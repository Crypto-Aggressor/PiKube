---
title: Monitoring with Prometheus
permalink: /documentation/4.1-monitoring-prometheus/
description: How to deploy kuberentes cluster monitoring solution based on Prometheus. Installation based on Prometheus Operator using kube-prometheus-stack project.
last_modified_at: "22-12-2023"
---

<p align="center">
    <img alt="prometheus"
    src="./resources/monitoring/monitoring-prometheus/prometheus-logo.svg"
    width="500%"
    height="%">
</p>

<hr>

<!-- omit in toc -->
# Monitoring with Prometheus

The installation of a **`Prometheus`** stack for Kubernetes can be efficiently conducted using the [**`kube-prometheus`**](https://github.com/prometheus-operator/kube-prometheus) project, which is maintained by the community.

This project aggregates Kubernetes manifests, Grafana dashboards, and Prometheus rules, along with comprehensive documentation and scripts. This collection is designed to facilitate a straightforward operational experience for end-to-end Kubernetes cluster monitoring with Prometheus, leveraging the Prometheus Operator.

**The kube-prometheus stack includes the following components**:

- [**`Prometheus Operator`**](https://github.com/prometheus-operator/prometheus-operator): Streamlines the deployment, configuration, and management of Prometheus and related monitoring components.

- [**`Highly Available Prometheus`**](https://prometheus.io/): Ensures Prometheus is consistently up and running, minimizing downtime and data loss.

- [**`Highly Available Alertmanager`**](https://github.com/prometheus/alertmanager): Manages and dispatches alerts generated by Prometheus, ensuring high availability.

- [**`Prometheus Node Exporter`**](https://github.com/prometheus/node_exporter): Gathers metrics from each node in the cluster, providing a detailed view of node-level statistics.

- [**`Kube-State-Metrics`**](https://github.com/kubernetes/kube-state-metrics): Collects metrics about the state of various Kubernetes objects, offering insights into the operational status of different components.

- [**`Grafana`**](https://grafana.com/): A powerful visualization tool that integrates with Prometheus to create insightful dashboards reflecting the cluster's health and performance.

This stack is specifically tailored for cluster monitoring, pre-configured to gather metrics from all Kubernetes components comprehensively.

Additionally, the architecture of the deployed components is illustrated in the below diagram, offering a visual understanding of how the system operates and interacts within a Kubernetes environment. This visual aid enhances the comprehensibility of the monitoring setup and its constituent elements.

<hr>

===
<!-- omit in toc -->
## Table of content

<div style="font-size:larger;">

[<strong>About the Prometheus Operator</strong>](#about-the-prometheus-operator)

[<strong>Kube-Prometheus Stack Installation via Helm Chart</strong>](#kube-prometheus-stack-installation-via-helm-chart)

</div>

## **About the Prometheus Operator**

The Prometheus Operator is designed to manage Prometheus and AlertManager deployments along with their configurations, utilizing Kubernetes CRDs (Custom Resource Definitions):

- **`Prometheus and AlertManager CRDs`**: These allow for the declarative specification of the desired setup for Prometheus and AlertManager within a Kubernetes cluster. They offer configuration options for aspects such as the number of replicas and persistent storage.

- **`ServiceMonitor, PodMonitor, and Probe CRDs`**: These manage the service discovery configurations for Prometheus. They define the way a dynamic set of services, pods, or static targets should be monitored, enhancing the flexibility and scalability of monitoring.

- **`PrometheusRules CRD`**: This defines the alerting and recording rules for Prometheus. Alerting rules specify the conditions under which alerts should be generated and sent (via AlertManager), while recording rules allow for the precomputation and storage of frequently needed or complex expressions as a new set of time series data.

- **`AlertManagerConfig CRD:`** This defines the configuration for AlertManager, enabling the routing of alerts to customized receivers and the establishment of inhibition rules, which control when certain alerts should be suppressed.

📌 **Note**

*For comprehensive details about the design and functionality of the Prometheus Operator and its CRDs, refer to the [**`Prometheus Operator Design Documentation`**](https://prometheus-operator.dev/docs/operator/design/).*

*The specifications for the various CRDs are detailed in the [**`Prometheus Operator API`**](https://prometheus-operator.dev/docs/operator/api/) reference guide, providing in-depth information for advanced configuration and usage.*

## **Kube-Prometheus Stack Installation via Helm Chart**

The Kube-Prometheus stack, maintained by the community, provides a comprehensive collection of monitoring resources, and it can be efficiently deployed using the Helm package manager.

- Start by adding the official Prometheus community repository to Helm

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
```

- Fetch the latest charts and updates from all added repositories

```bash
helm repo update
```

- Establish a dedicated namespace for all your monitoring components

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml create namespace monitoring
```

- Create **`prometeus-values.yaml`** file with the desired configuration for Kube-Prometheus stack. Below is a detailed breakdown of the key configurations:

  - **Prometheus Operator**:

  ➜ ServiceMonitor: Configures the relabeling of metrics and disables the kubelet service.

  - **Alertmanager**:

  ➜ Sets up a persistent volume using Longhorn and configures Alertmanager to run under a specific subpath. Routes and storage specifications are defined.

  **Prometheus**:

  ➜ Details memory resources, storage, and retention period. Configures Prometheus to run under a subpath and removes default selector filters to allow more flexible monitoring setups.

  - **Grafana**:

  ➜ Configures the domain, subpath, admin password, and plugins. Sets up service monitor labeling and relabeling. Integrates with Loki and configures the dashboard sidecar for searching across all namespaces.

  - **Kubernetes Component Monitoring**:

  ➜ Disables monitoring for default Kubernetes components since K3S components will be monitored separately.

  - **Service Monitors and Selectors**:

  ➜ Adjusts default filters and relabeling for Prometheus, Prometheus Operator, and Grafana monitoring.

```yaml
# Prometheus Operator configuration
prometheusOperator:
  serviceMonitor:
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: prometheus-operator
  kubeletService:
    enabled: false

# Alertmanager configuration
alertmanager:
  alertmanagerSpec:
    externalUrl: http://monitor.picluster.quantfinancehub.com/alertmanager/
    routePrefix: /
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
  serviceMonitor:
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: alertmanager

# Prometheus configuration
prometheus:
  prometheusSpec:
    externalUrl: http://monitoring.picluster.quantfinancehub.com/prometheus/
    routePrefix: /
    resources:
      requests:
        memory: 1Gi
      limits:
        memory: 1Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    retention: 7d
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
  serviceMonitor:
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: prometheus

# Grafana configuration
grafana:
  grafana.ini:
    server:
      domain: monitoring.picluster.quantfinancehub.com
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
      serve_from_sub_path: true
  adminPassword: "admin_password"
  plugins:
    - grafana-piechart-panel
  serviceMonitor:
    labels:
      release: kube-prometheus-stack
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: grafana
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki-gateway.logging.svc.cluster.local
  sidecar:
    dashboards:
      searchNamespace: ALL

# Disabling default Kubernetes service monitoring
kubelet: { enabled: false }
kubeApiServer: { enabled: false }
kubeControllerManager: { enabled: false }
kubeScheduler: { enabled: false }
kubeProxy: { enabled: false }
kubeEtcd: { enabled: false }

# Disabling default K8S Prometheus Rules
defaultRules:
  create: true
  rules:
    etcd: false
    k8s: false
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: false
    kubeProxy: false
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: false
    kubeScheduler: false
```

- Deploy the **`Kube-Prometheus`** stack in the **`monitoring`** namespace using the Helm chart and the custom **`prometeus-values.yaml`**

```bash
helm --kubeconfig=/home/pi/.kube/config.yaml install -f prometeus-values.yaml kube-prometheus-stack prometheus-community/kube-prometheus-stack --namespace monitoring
```

## **Configuring Ingress Resources for Accessing Monitoring Services**

Facilitate external access to Prometheus, Grafana, and AlertManager services through an Ingress Controller. Instead of employing separate DNS domains for each component, they are configured to operate behind an NGINX HTTP Proxy using a single domain, monitoring.picluster.quantfinancehub.com, with unique subpaths:

**`Grafana`**:

https://monitoring.picluster.quantfinancehub.com/grafana/

**`Prometheus`**:

https://monitoring.picluster.quantfinancehub.com/prometheus/

**`AlertManager`**:

https://monitoring.picluster.quantfinancehub.com/alertmanager/

The DNS domain **`monitoring.picluster.quantfinancehub.com`** must be mapped in the cluster DNS server configuration to the external IP of the NGINX Load Balancer service.

As the backend services for Prometheus, Grafana, and AlertManager don't provide secure communications (HTTP traffic) by default, the Ingress resource will be configured to enforce HTTPS (NGINX TLS endpoint) and redirect all HTTP traffic to HTTPS. Additionally, since Prometheus and AlertManager frontends lack native authentication mechanisms, NGINX HTTP basic authentication will be employed for security.

Ingress NGINX rewrite rules are defined within the Ingress resources.

- Create Ingress Resources Manifest **`prometheus-monitoring-ingress.yaml`**

```yaml
# Ingress for Grafana
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-grafana
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    cert-manager.io/cluster-issuer: letsencrypt-issuer
    cert-manager.io/common-name: monitoring.picluster.quantfinancehub.com
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - monitoring.picluster.quantfinancehub.com
      secretName: monitoring-tls
  rules:
    - host: monitoring.picluster.quantfinancehub.com
      http:
        paths:
          - path: /grafana/(.*)
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-stack-grafana
                port:
                  number: 80
---
# Ingress for Prometheus
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-prometheus
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: nginx/basic-auth-secret
    cert-manager.io/cluster-issuer: letsencrypt-issuer
    cert-manager.io/common-name: monitoring.picluster.quantfinancehub.com
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - monitoring.picluster.quantfinancehub.com
      secretName: monitoring-tls
  rules:
    - host: monitoring.picluster.quantfinancehub.com
      http:
        paths:
          - path: /prometheus/(.*)
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-stack-prometheus
                port:
                  number: 9090
---
# Ingress for AlertManager
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-alertmanager
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    cert-manager.io/cluster-issuer: letsencrypt-issuer
    cert-manager.io/common-name: monitoring.picluster.quantfinancehub.com
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - monitoring.picluster.quantfinancehub.com
      secretName: monitoring-tls
  rules:
    - host: monitoring.picluster.quantfinancehub.com
      http:
        paths:
          - path: /alertmanager/(.*)
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-stack-alertmanager
                port:
                  number: 9093
```

- Deploy the Ingress configuration to your cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml apply -f prometheus-monitoring-ingress.yaml
```

## Detailed Breakdown of what been deployed by kube-stack

### Detailed Breakdown of Prometheus Custom Resource

The kube-stack deployment method efficiently sets up a Prometheus Operator in PiKube Kubernetes cluster. This process involves deploying the Prometheus Operator and creating the necessary **`Prometheus`** and **`AlertManager`** objects. These objects instruct the operator to roll out the corresponding Prometheus and AlertManager pods as StatefulSets.

📌 **Note**

*The final configuration can be customized through helm chart values (**`prometheus.prometheusSpec`** and **`alertmanager.alertmanagerSpec`**).*

The **`Prometheus custom resource`** (CR) is a crucial part of this setup. It contains the desired configuration for the Prometheus server.

It can be retreived from PiKube Kunbernetes cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get prometheus -n monitoring
kubectl --kubeconfig=/home/pi/.kube/config.yaml get prometheus kube-prometheus-stack-prometheus -n monitoring -o yaml
```

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:48Z"
  generation: 1
  labels:
    app: kube-prometheus-stack-prometheus
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    heritage: Helm
    release: kube-prometheus-stack
  name: kube-prometheus-stack-prometheus
  namespace: monitoring
  resourceVersion: "1378214"
  uid: 73845123-041c-4b2e-bf15-b64f40b0a6ed
spec:
  alerting:
    alertmanagers:
    - apiVersion: v2
      name: kube-prometheus-stack-alertmanager
      namespace: monitoring
      pathPrefix: /
      port: http-web
  enableAdminAPI: false
  evaluationInterval: 30s
  externalUrl: http://monitoring.picluster.quantfinancehub.com/prometheus/
  hostNetwork: false
  image: quay.io/prometheus/prometheus:v2.48.1
  listenLocal: false
  logFormat: logfmt
  logLevel: info
  paused: false
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  portName: http-web
  probeNamespaceSelector: {}
  probeSelector: {}
  replicas: 1
  resources:
    limits:
      memory: 1Gi
    requests:
      memory: 1Gi
  retention: 7d
  routePrefix: /
  ruleNamespaceSelector: {}
  ruleSelector: {}
  scrapeConfigNamespaceSelector: {}
  scrapeConfigSelector:
    matchLabels:
      release: kube-prometheus-stack
  scrapeInterval: 30s
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  serviceAccountName: kube-prometheus-stack-prometheus
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  shards: 1
  storage:
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        storageClassName: longhorn
  tsdb:
    outOfOrderTimeWindow: 0s
  version: v2.48.1
  walCompression: true
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2023-12-22T20:58:40Z"
    message: ""
    observedGeneration: 1
    reason: ""
    status: "True"
    type: Available
  - lastTransitionTime: "2023-12-22T20:58:40Z"
    message: ""
    observedGeneration: 1
    reason: ""
    status: "True"
    type: Reconciled
  paused: false
  replicas: 1
  shardStatuses:
  - availableReplicas: 1
    replicas: 1
    shardID: "0"
    unavailableReplicas: 0
    updatedReplicas: 1
  unavailableReplicas: 0
  updatedReplicas: 1
```

**Specifications of the Prometheus Custom Resource**:

**`apiVersion & kind`**: These fields identify the resource as a Prometheus object under the monitoring.coreos.com/v1 API.

**`metadata`**: This section provides the naming and labeling information for the resource, crucial for identification and management.

**`spec`**: This section is the heart of the resource, defining how Prometheus should run and behave:

**`alerting`**: Outlines how alerts should be managed and where they should be sent, specifying the AlertManager service details.

**`version & image`**: Specifies the Prometheus version and the container image to use, ensuring you're deploying a specific, known version.

**`replicas & shards`**: Dictates the high availability configuration, with replicas for redundancy and shards for distributing load.

**`scrapeInterval & evaluationInterval`**: Defines how often Prometheus scrapes targets and evaluates rules, crucial for timely data collection and alerting.

**`retention`**: Determines how long data is kept, balancing between historical insight and storage utilization.

**`storage`**: Specifies the persistent storage configuration, vital for data persistence across pod restarts and upgrades.

**`serviceMonitorSelector, podMonitorSelector, probeSelector`**: These selectors determine which monitor objects Prometheus should pay attention to, based on their labels.

> 💡 **Additional Considerations**
>
> **`HA Mechanisms`**: While basic HA is achieved through replication (running multiple instances), advanced > setups might use sharding. However, sharding requires additional components like Thanos for comprehensive querying and rule evaluation.

**`Resource and Storage Needs`**: Given the resource-intensive nature of Prometheus, especially in larger environments, ensure the cluster has the necessary resources and storage configured.

**`Alerting and Monitoring Scope`**: The setup connects to a specific AlertManager and monitors targets based on label selectors. Adjust these to fit the exact scope of the wanted monitoring and alerting on.

**`Customization and Flexibility`**: The default setup is designed for a general use case. However, there is possibility and a need to modify the helm values and resource definitions to tailor the setup to the specific needs and constraints.

### Detailed Breakdown of AlertManager Custom Resource

The kube-stack deployment process not only sets up the Prometheus Operator but also creates and configures the necessary AlertManager objects. These objects guide the operator to deploy AlertManager pods as StatefulSets, ensuring a robust and responsive alerting system.

The AlertManager custom resource (CR) defines the desired state and configuration of the AlertManager server.

It can be retreived from PiKube Kunbernetes cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get alertmanager -n monitoring
kubectl --kubeconfig=/home/pi/.kube/config.yaml get alertmanager <name-of-alertmanager-cr> -n monitoring -o yaml
```

```yaml
apiVersion: monitoring.coreos.com/v1                                                     
kind: Alertmanager                                                                       
metadata:                                                                                
  annotations:                                                                           
    meta.helm.sh/release-name: kube-prometheus-stack                                     
    meta.helm.sh/release-namespace: monitoring                                           
  creationTimestamp: "2023-12-22T19:54:48Z"                                              
  generation: 1                                                                          
  labels:                                                                                
    app: kube-prometheus-stack-alertmanager                                              
    app.kubernetes.io/instance: kube-prometheus-stack                                    
    app.kubernetes.io/managed-by: Helm                                                   
    app.kubernetes.io/part-of: kube-prometheus-stack                                     
    app.kubernetes.io/version: 55.5.0                                                    
    chart: kube-prometheus-stack-55.5.0                                                  
    heritage: Helm                                                                       
    release: kube-prometheus-stack                                                       
  name: kube-prometheus-stack-alertmanager                                               
  namespace: monitoring                                                                  
  resourceVersion: "1345806"                                                             
  uid: 763c17c3-8709-47fb-a839-2cc0310aa8fb                                              
spec:                                                                                    
  alertmanagerConfigNamespaceSelector: {}                                                
  alertmanagerConfigSelector: {}                                                         
  externalUrl: http://monitor.picluster.quantfinancehub.com/alertmanager/                
  image: quay.io/prometheus/alertmanager:v0.26.0                                         
  listenLocal: false                                                                     
  logFormat: logfmt                                                                      
  logLevel: info                                                                         
  paused: false                                                                          
  portName: http-web                                                                     
  replicas: 1                                                                            
  retention: 120h                                                                        
  routePrefix: /                                                                         
  securityContext:                                                                       
    fsGroup: 2000                                                                        
    runAsGroup: 2000                                                                     
    runAsNonRoot: true                                                                   
    runAsUser: 1000                                                                      
    seccompProfile:                                                                      
      type: RuntimeDefault                                                               
  serviceAccountName: kube-prometheus-stack-alertmanager                                 
  storage:                                                                               
    volumeClaimTemplate:                                                                 
      spec:                                                                              
        accessModes:                                                                     
        - ReadWriteOnce                                                                  
        resources:                                                                       
          requests:                                                                      
            storage: 50Gi                                                                
        storageClassName: longhorn                                                       
  version: v0.26.0                                                                       
status:                                                                                  
  availableReplicas: 1                                                                   
  conditions:                                                                            
  - lastTransitionTime: "2023-12-22T19:56:39Z"                                           
    message: ""                                                                          
    observedGeneration: 1                                                                
    reason: ""                                                                           
    status: "True"                                                                       
    type: Available                                                                      
  - lastTransitionTime: "2023-12-22T19:55:04Z"                                           
    message: ""                                                                          
    observedGeneration: 1                                                                
    reason: ""                                                                           
    status: "True"                                                                       
    type: Reconciled                                                                     
  paused: false                                                                          
  replicas: 1                                                                            
  unavailableReplicas: 0                                                                 
  updatedReplicas: 1
```

**Specifications of the AlertManager Custom Resource**:

**`apiVersion & kind`**: Specifies the resource type as Alertmanager under the monitoring.coreos.com/v1 API.

**`metadata`**: Provides naming and labeling details essential for resource management and identification.

**`spec`**: Describes how AlertManager should run and behave:

**`version & image`**: Dictates the AlertManager version and image, promoting consistency and reliability.

**`replicas`**: Indicates the number of AlertManager instances, pivotal for high availability.

**`retention`**: Sets how long AlertManager retains its data, balancing between availability and storage management.

**`storage`**: Outlines the persistent storage configuration to ensure data durability.

**`externalUrl`**: Specifies the external URL where AlertManager is accessible, vital for user interaction and integrations.

> 💡 **Additional Considerations**
>
> **`High Availability`**: The configuration can be scaled up to multiple replicas to enhance reliability and ensure continuous monitoring.
>
> **`Storage Requirements`**: Given AlertManager's role in managing alerts, ensure the cluster has the necessary resources and storage as defined.
>
> **`Integration with Prometheus`**: AlertManager works in tandem with Prometheus to handle alerts efficiently. Ensure it's correctly linked to the Prometheus instances.

### Detailed Breakdown of ServiceMonitor Objects

**`The kube-prometheus-stack`** deployment automatically creates several ServiceMonitor objects. These objects are crucial as they define how Prometheus should discover and scrape metrics from various components:

**`Core Components`**: Includes **`Node Exporter`**, **`Grafana`**, **`Kube-State-Metrics`**, **`Prometheus`** itself, **`Prometheus Operator`** and **`AlertManager`**.

**`Kubernetes Services and Processes`**: Depending on the helm chart configuration, it might include **`coreDNS`**, **`Kube Api server`**, **`kubelet`**, **`Kube Controller Manager`**, **`Kubernetes Scheduler`**, **`Kubernetes etcd`**, and **`Kube Proxy`**.

> 📌 **Note**
>
> *In the chart's configuration, monitoring for the **`kube-controller-manager`**, **`kube-scheduler`**, **`kube-proxy`**, and **`kubelet`** components has been deactivated. However, the **`coreDNS`** component's monitoring remains active.*
>
> *Refer to thw below [**`K3S components monitoring`**]() section to understand the rationale behind the disabling of Kubernetes components monitoring in the kube-prometheus-stack and for guidance on how to manually set up monitoring for K3s.*

### Detailed Breakdown of PrometheusRule Objects

Alongside ServiceMonitors, the **`kube-prometheus-stack`** generates PrometheusRule objects. These define the rules under which alerts are generated and the metrics Prometheus records:

**`Generated Rules`**: Include a comprehensive set of rules covering various aspects of the cluster's health and performance.

**`Customization`**: Rules can be added or modified to suit the specific monitoring and alerting needs.

### Detailed Breakdown of Grafana

Grafana is deployed as a subchart of the kube-prometheus-stack, and its configuration is passed via the [**`Grafana value`**](https://github.com/grafana/helm-charts/tree/main/charts/grafana) in the helm chart.

It can be retreived from PiKube Kunbernetes cluster

```bash
helm --kubeconfig=/home/pi/.kube/config.yaml list -n monitoring
helm --kubeconfig=/home/pi/.kube/config.yaml get values kube-prometheus-stack -n monitoring
```

```yaml
grafana:
  additionalDataSources:
  - name: Loki
    type: loki
    url: http://loki-gateway.logging.svc.cluster.local
  adminPassword: admin_password
  grafana.ini:
    server:
      domain: monitoring.picluster.quantfinancehub.com
      root_url: '%(protocol)s://%(domain)s:%(http_port)s/grafana/'
      serve_from_sub_path: true
  plugins:
  - grafana-piechart-panel
  serviceMonitor:
    labels:
      release: kube-prometheus-stack
    relabelings:
    - action: replace
      replacement: grafana
      sourceLabels:
      - __address__
      targetLabel: job
  sidecar:
    dashboards:
      searchNamespace: ALL
kubeApiServer:
  enabled: false
kubeControllerManager:
  enabled: false
kubeEtcd:
  enabled: false
kubeProxy:
  enabled: false
kubeScheduler:
  enabled: false
kubelet:
  enabled: false
```

Key configurations include:

**`Frontend Setup`**: Configured to run behind an HTTP proxy at the /grafana subpath.

**`Authentication`**: Specifies the admin password for secure access.

**`Plugins`**: Includes essential plugins like the grafana-piechart-panel.

**`Additional Data Sources`**: Integrates additional data sources such as Loki for log aggregation.

**`ServiceMonitor & Sidecar Configuration`**: Sets up ServiceMonitor labels and job relabeling along with additional sidecar dashboard provisioner configurations.

> 💡 **Additional Considerations**
>
> **`Deployment Customization`**: Both AlertManager and Grafana offer numerous customization options. Tailor these settings to align with the operational requirements and constraints.
>
> **`Integration and Verification`**: Once deployed, ensure all components are correctly integrated and functioning as expected. Regular checks and updates can help maintain a healthy monitoring environment.
>
> **`Resource Management`**: Monitor the resource usage and performance of AlertManager and Grafana, adjusting configurations as necessary to ensure optimal operation.

### Provisioning Grafana Dashboards Automatically

[**`Grafana dashboards`**](https://grafana.com/docs/grafana/latest/dashboards/) can be set up through provider definitions in YAML files located in the provisioning directory (**`/etc/grafana/provisioning/dashboards`**). These files specify the directory from which to load JSON-formatted dashboards. For more details, refer to the Grafana Tutorial on [**`Provisioning Dashboards and Data Sources`**](https://grafana.com/tutorials/provision-dashboards-and-data-sources/).

When deploying Grafana in Kubernetes via Helm, dashboards can be provisioned automatically by enabling a sidecar container provisioner. The Grafana Helm chart generates a **`/etc/grafana/provisioning/dashboard/provider.yml`** file, instructing Grafana to load all JSON dashboards from **`/tmp/dashboards`**.

The configuration in **`provider.yml`** is:

```yaml
apiVersion: 1
providers:
  - name: 'sidecarProvider'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    allowUiUpdates: false
    updateIntervalSeconds: 30
    options:
      foldersFromFilesStructure: false
      path: /tmp/dashboards
```

With the sidecar provider activated, Grafana dashboards are automatically provisioned by creating ConfigMap resources that contain the dashboard's JSON definition. The provisioning sidecar container must be enabled to monitor these ConfigMaps in real time and copy them to the provisioning directory (**`/tmp/dashboards`**).

The **`kube-prometheus-stack`**, by default, configures the Grafana provisioning sidecar to look for new ConfigMaps labeled with **`grafana_dashboard`**. The default Helm chart values for the kube-prometheus-stack are:

```yaml
grafana:
  sidecar:
    dashboards:
      SCProvider: true
      enabled: true
      folder: /tmp/dashboards
      label: grafana_dashboard
```

To automatically provision a new dashboard, create a **`ConfigMap`** resource labeled with **`grafana_dashboard: "1"`** and include the JSON file content as **`data`**:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: sample-grafana-dashboard
  labels:
     grafana_dashboard: "1"
data:
  dashboard.json: |-
    [json_file_content]
```

To enable the search for ConfigMaps across all namespaces (the default is limited to the Grafana namespace), additional Helm chart configuration is required:

```yaml
grafana:
  sidecar:
    dashboards:
      searchNamespace: ALL
```

Following this setup, the kube-prometheus-stack Helm chart automatically deploys a set of dashboards for monitoring metrics from Kubernetes processes and Node Exporter.

However, for K8s components disabled in the chart configuration, the corresponding dashboards are not deployed and need to be added manually. Refer to the lis of [**`kube-prometheus-stack grafana dashboards`**](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/grafana/dashboards-1.14)

To manually add these dashboards or to retrieve all deployed dashboards, use the following command:

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get cm -l "grafana_dashboard=1" -n monitoring
```

> 🚨 **Important Note**
>
> Many [**`Grafana community dashboards`**](https://grafana.com/grafana/dashboards/) are exported from a running instance and include an input variable (**`DS_PROMETHEUS`**) referenced in all dashboard panels. When provisioning these dashboards automatically, it possible to encounter an error stating **`"Datasource named ${DS_PROMETHEUS} was not found."`** This is due to an open Grafana issue requesting support for dashboard variables in provisioning. 
>
> A workaround is to modify the JSON files before inserting them into the ConfigMap YAML file, detecting the **`DS_PROMETHEUS`** datasource and adjusting the **`templating.list`** key accordingly.
>
> ```json
> "templating": {
>     "list": [
>       {
>         "hide": 0,
>         "label": "datasource",
>         "name": "DS_PROMETHEUS",
>         "options": [],
>         "query": "prometheus",
>         "refresh": 1,
>         "regex": "",
>         "type": "datasource"
>       },
>     ...
> ```

### Automatic Provisioning of Grafana DataSources

Grafana allows for the configuration of [**`data sources`**](https://grafana.com/docs/grafana/latest/datasources/) through YAML files situated in the provisioning directory (**`/etc/grafana/provisioning/datasources`**). For a comprehensive guide, refer to the Grafana Tutorial on [**`Provisioning Dashboards and Data Sources`**](https://grafana.com/tutorials/provision-dashboards-and-data-sources/).

In a Kubernetes deployment of Grafana, data source configuration files can be introduced from ConfigMaps. This process is facilitated by a sidecar container that translocates these ConfigMaps into its provisioning directory.

For detailed instructions on enabling and utilizing this sidecar container, see the [**`“Grafana chart documentation: Sidecar for Datasources”`**](https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-datasources).

By default, the **`kube-prometheus-stack`** activates the Grafana data source sidecar to monitor for new ConfigMaps tagged with the label grafana_datasource. The configuration for this is as follows:

```yaml
sidecar:
  datasources:
    enabled: true
    defaultDatasourceEnabled: true
    uid: prometheus
    annotations: {}
    createPrometheusReplicasDatasources: false
    label: grafana_datasource
    labelValue: "1"
    exemplarTraceIdDestinations: {}
```

Below is an example of a ConfigMap automatically generated by the **`kube-prometheus-stack`**, defining the data source that connects Grafana to the Prometheus server (named "**`Prometheus`**"):

It can be retreived from PiKube Kunbernetes cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get configmaps -n monitoring
kubectl --kubeconfig=/home/pi/.kube/config.yaml get configmap <ConfigMap-Name> -n monitoring -o yaml # similar to kube-prometheus-stack-grafana-datasource 
```

```yaml
apiVersion: v1
data:
  datasource.yaml: |-
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      uid: prometheus
      url: http://kube-prometheus-stack-prometheus.monitoring:9090/
      access: proxy
      isDefault: true
      jsonData:
        httpMethod: POST
        timeInterval: 30s
    - name: Alertmanager
      type: alertmanager
      uid: alertmanager
      url: http://kube-prometheus-stack-alertmanager.monitoring:9093/
      access: proxy
      jsonData:
        handleGrafanaManagedAlerts: false
        implementation: prometheus
    - name: Loki
      type: loki
      url: http://loki-gateway.logging.svc.cluster.local
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:47Z"
  labels:
    app: kube-prometheus-stack-grafana
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    grafana_datasource: "1"
    heritage: Helm
    release: kube-prometheus-stack
  name: kube-prometheus-stack-grafana-datasource
  namespace: monitoring
  resourceVersion: "1344466"
  uid: 78b4f328-50dd-40d8-b03a-27b3a8da344b
```

This ConfigMap is distinguished by the **`grafana_datasource`** label, which ensures it's recognized and imported by the sidecar container into Grafana’s provisioning directory, thereby streamlining the setup process.

### Prometheus Node Exporter Deployment

The Prometheus Node Exporter, deployed via its [**`Helm chart`**](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter), is included as a subchart within the kube-prometheus-stack Helm chart. This deployment facilitates the Node Exporter's installation across all cluster nodes as a DaemonSet.

The kube-prometheus-stack Helm chart utilizes the **`prometheus-node-exporter`** values to configure the Node Exporter chart. By default, the values.yml file in the kube-prometheus-stack contains specific configurations that remain unchanged in the [**`standard installation procedure`**](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml).

Here's an outline of the default settings for the Prometheus Node Exporter within the kube-prometheus-stack:

```yaml
prometheus-node-exporter:
  # Namespace override
  namespaceOverride: ""
  
  # Pod labels
  podLabels:
    jobLabel: node-exporter  # Label for serviceMonitor to match standard usage in rules and Grafana dashboards
  
  # Additional arguments to exclude certain mount points and file types
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  
  # Service and monitoring configurations
  service:
    portName: http-metrics
  prometheus:
    monitor:
      enabled: true
      jobLabel: jobLabel
      interval: ""  # Scrape interval
      scrapeTimeout: ""  # Scrape request timeout
      proxyUrl: ""  # Proxy URL for scraping
      metricRelabelings: []  # Metric relabeling configurations
      relabelings: []  # Relabeling configurations before scraping
  
  # RBAC configuration
  rbac:
    pspEnabled: false  # Create PodSecurityPolicies for node-exporte
```

The default configuration primarily focuses on excluding specific mount points and file types from monitoring (**`extraArgs`**) and establishes a ServiceMonitor object to initiate metric scraping from the exporter.

Metrics from the Prometheus Node Exporter are available at the **`/metrics`** endpoint on TCP port 9100 of each DaemonSet Pod. This setup ensures comprehensive node-level metrics collection across the Kubernetes cluster, aiding in detailed monitoring and analysis.

### Deployment of Kube State Metrics

The [**`Prometheus Kube State Metrics`**](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics), integrated via its dedicated Helm chart, is included as a subchart in the kube-prometheus-stack Helm chart deployment.

This chart facilitates the deployment of the [**`kube-state-metrics agent`**](https://github.com/kubernetes/kube-state-metrics). Kube-state-metrics (KSM) is a straightforward service that monitors the Kubernetes API server and generates metrics reflecting the status of various Kubernetes objects.

The configuration for **`kube-state-metrics`** within the kube-prometheus-stack is provided via the **`kube-state-metrics value`**, which is passed to the kube-state-metrics chart to dictate its setup and behavior.

The metrics from kube-state-metrics are made available on TCP port 8080 at the **`/metrics`** endpoint, providing a comprehensive set of data about the state and performance of different Kubernetes resources and objects.

## Monitoring K3S and Cluster Services

This section outlines the steps required to enable Prometheus monitoring for K3S components and various deployed cluster services.

The process involves setting up Kubernetes resources such as **`Services`**, **`Endpoints`**, **`ServiceMonitors`**, **`PodMonitors`**, and **`Probes`**. These are necessary to configure Prometheus' service discovery and monitoring capabilities. Additionally, it details how to import dashboards in JSON format into Grafana for a visual representation of the metrics from each specific service.

> 📌 **Note**
>
> *To provision the dashboards mentioned in the subsequent sections, a ConfigMap should be created for each dashboard (JSON file). This can be done following the process previously described:*
>
> ```yaml
> apiVersion: v1
> kind: ConfigMap
> metadata:
>   name: sample-grafana-dashboard
>   labels:
>      grafana_dashboard: "1"
> data:
>   dashboard.json: |-
>     [json_file_content]
> ```

### K3S Components Monitoring

The [**`Kubernetes Documentation on System Metrics`**](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/) outlines various components that expose metrics in a Prometheus-friendly format:

- **`kube-controller-manager`**: Metrics endpoint at TCP 10257
- **`kube-proxy`**: Metrics endpoint at TCP 10249
- **`kube-apiserver`**: Metrics at Kubernetes API port TCP 6443
- **`kube-scheduler`**: Metrics endpoint at TCP 10259
- **`kubelet`**: Metrics at **`/metrics`**, **`/metrics/cadvisor`**, **`/metrics/resource`**, and **`/metrics/probes`** endpoints at TCP 10250

> 📌 **Note**
>
> *From Kubernetes release 1.22, TCP ports exposed by kube-scheduler and kube-controller-manager have changed. Additionally, HTTPS authenticated connections are now required, necessitating authorized service accounts for metrics access.*
>
>🚨 **Important Note**
>
> By default, K3S components do not expose their metrics endpoints for remote querying. To alter this and allow for remote metrics collection, include the following arguments in the K3S installation:
>
> ```bash
> --kube-controller-manager-arg 'bind-address=0.0.0.0' 
> --kube-proxy-arg 'metrics-bind-address=0.0.0.0'
> --kube-scheduler-arg 'bind-address=0.0.0.0'
> ```

However, the default resources created by kube-prometheus-stack to scrape metrics from all Kubernetes components are not suitable for a K3S cluster due to its unique behavior concerning metrics exposure. K3S emits the same metrics across all ports for each node, leading to potential duplicates in Prometheus.

To avoid duplicate metrics and unnecessary resource consumption, it's recommended to scrape only the kubelet metrics endpoints (TCP 10250). This includes **`/metrics`**, **`/metrics/cadvisor`**, **`/metrics/resource`**, and **`/metrics/probes`**.

As a result, the monitoring of Kubernetes components has been disabled in the kube-prometheus-stack chart configuration, and a manual resource configuraiton is needed to scrape metrics from the kubelet endpoints.

```yaml
# Disabling default Kubernetes service monitoring
kubelet: { enabled: false }
kubeApiServer: { enabled: false }
kubeControllerManager: { enabled: false }
kubeScheduler: { enabled: false }
kubeProxy: { enabled: false }
kubeEtcd: { enabled: false }

# Disabling default K8S Prometheus Rules
defaultRules:
  create: true
  rules:
    etcd: false
    k8s: false
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: false
    kubeProxy: false
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: false
    kubeScheduler: false
```

Create a **`prometheus-k3s-metrics-service.yaml`** manifest file to set up a headless service for K3S metrics. This service should be a **`ClusterIP`** with **`clusterIP: None`**, allowing Prometheus to discover individual pods behind the service. Since metrics are emitted by the k3s process rather than pods, the service must be defined without a selector, and endpoints must be explicitly defined.

```yaml
---
# Headless service for K3S metrics. No selector.
apiVersion: v1
kind: Service
metadata:
  name: k3s-metrics-service
  labels:
    app.kubernetes.io/name: kubelet
  namespace: kube-system
spec:
  clusterIP: None
  ports:
  - name: https-metrics
    port: 10250
    protocol: TCP
    targetPort: 10250
  type: ClusterIP
---
# Endpoints for the headless service without a selector.
apiVersion: v1
kind: Endpoints
metadata:
  name: k3s-metrics-service
  namespace: kube-system
subsets:
- addresses:
  - ip: 10.0.0.10
  - ip: 10.0.0.11
  - ip: 10.0.0.12
  - ip: 10.0.0.13
  - ip: 10.0.0.14
  - ip: 10.0.0.15
  - ip: 10.0.0.16
  ports:
  - name: https-metrics
    port: 10250
    protocol: TCP
```

Next, create a **`ServiceMonitor`** resource, named **`prometheus-k3s-servicemonitor.yaml`** to let Prometheus discover these targets. This single ServiceMonitor should enable the collection of all Kubernetes component metrics from the unique port TCP 10250, including necessary relabeling and dropping rules.

> 📌 **Note**
>
> This ServiceMonitor configures a single Prometheus scraping job with the label **`job="kubelet"`**. Most Grafana dashboards use kubelet metrics, and their PromQL queries filter metrics by the label **`job="kubelet"`**.
> When the monitoring of Kubernetes components is deactivated within the kube-prometheus-stack, the associated Prometheus rules for Kubernetes components are not installed. Moreover, these rules are not applicable for K3S as they include PromQL queries that filter metrics based on job labels like “apiserver,” “kubelet,” etc.

By default, the kube-prometheus-stack generates various PrometheusRules resources. These are collectively contained within a single manifest file named [**`kubernetesControlPlane-prometheusRule.yaml`**](https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/kubernetesControlPlane-prometheusRule.yaml) in the prometheus-operator's source repository.

To adapt these rules for K3S, modifications are required in the yaml file to change the job labels:

Update the job labels from:

- **`job="apiserver"`**
- **`job="kube-proxy"`**
- **`job="kube-scheduler"`**
- **`job="kube-controller-manager"`**

to simply:

- **`job="kubelet"`**

This aligns the rules with the K3S metrics that are all emitted under the **`kubelet`** job label.

To ensure these modified rules are recognized and used by the Prometheus Operator, replace specific label to the metadata section of the PrometheusRule resource:

from

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app.kubernetes.io/name: kube-prometheus
    app.kubernetes.io/part-of: kube-prometheus
    prometheus: k8s
    role: alert-rules
  name: kubernetes-monitoring-rules
```

to

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    release: kube-prometheus-stack
  name: kubernetes-monitoring-rules
```

This label assists the Prometheus Operator in identifying and applying the rules appropriately.

- Apply manifest files

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml apply -f prometheus-k3s-metrics-service.yaml
kubectl --kubeconfig=/home/pi/.kube/config.yaml apply -f prometheus-k3s-servicemonitor.yaml
kubectl --kubeconfig=/home/pi/.kube/config.yaml apply -f kubernetesControlPlane-prometheusRule.yaml
```

### coreDNS Monitoring

By default, the kube-prometheus-stack Helm chart enables CoreDNS monitoring.

```yaml
coreDns:
  enabled: true
  service:
    port: 9153
    targetPort: 9153
    ...
```

It creates a service named **`kube-prometheus-stack-coredns`** in the kube-system namespace that points to the CoreDNS Pod. Here's how to retreive it and an overview of the configuration:

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get svc kube-prometheus-stack-coredns -n kube-system -o yaml
```

```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:47Z"
  labels:
    app: kube-prometheus-stack-coredns
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    heritage: Helm
    jobLabel: coredns
    release: kube-prometheus-stack
  name: kube-prometheus-stack-coredns
  namespace: kube-system
  resourceVersion: "1344504"
  uid: b4453d96-1abe-4b0b-baea-36d5323540e9
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http-metrics
    port: 9153
    protocol: TCP
    targetPort: 9153
  selector:
    k8s-app: kube-dns
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Creates a ServiceMonitor. Here's Here's how to retreive it and an overview of the configuration:

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get servicemonitor kube-prometheus-stack-coredns -n monitoring -o yaml
```

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:48Z"
  generation: 1
  labels:
    app: kube-prometheus-stack-coredns
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    heritage: Helm
    release: kube-prometheus-stack
  name: kube-prometheus-stack-coredns
  namespace: monitoring
  resourceVersion: "1344626"
  uid: b8db049f-591b-4cf1-9ee0-45be43b72549
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    port: http-metrics
  jobLabel: jobLabel
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      app: kube-prometheus-stack-coredns
      release: kube-prometheus-stack
```

### K3S Grafana Dashboards

While kube-prometheus-stack installs Grafana dashboards for various Kubernetes components, a manual installation might be needed if their monitoring is disabled in the Helm chart configuration.

Dashboards to Download from [**`Grafana.com`**](https://grafana.com/):

- **`kubelet`**: ID [**`16361`**](https://grafana.com/grafana/dashboards/16361-kubernetes-kubelet/)
- **`apiserver`**: ID [**`12654`**](https://grafana.com/grafana/dashboards/12654-kubernetes-api-server/)
- **`etcd`**: ID [**`16359`**](https://grafana.com/grafana/dashboards/16359-etcd/)
- **`kube-scheduler`**: ID [**`12130`**](https://grafana.com/grafana/dashboards/12130-kubernetes-scheduler/)
- **`kube-controller-manager`**: ID [**`12122`**](https://grafana.com/grafana/dashboards/12122-kubernetes-controller-manager/)
- **`kube-proxy`**: ID [**`12129`**](https://grafana.com/grafana/dashboards/12129-kubernetes-proxy/)

**Modifications for K3S**:

The downloaded Grafana dashboards might need modifications as the default promQL queries use job name labels (like kube-scheduler, kube-proxy, apiserver, etc.) which are not used in K3S configuration. Instead, K3S typically has a single scraping job (kubelet) configured to scrape metrics from all components.

Changes to Apply in JSON Files:

Replace all instances of:

- **`job="apiserver"`**
- **`job="kube-proxy"`**
- **`job="kube-scheduler"`**
- **`job="kube-controller-manager"`**

With:

- **`job="kubelet"`**

Ensure to review the modified dashboards in Grafana to confirm they are reflecting the correct metrics after the changes.

##